\section{Getting Started}
\label{sec:getting-started}

Probabilistic modeling in Edward uses a simple language of
random variables.
Here we will show a Bayesian neural network. It is a neural network
with a prior distribution on its weights.

First, simulate a toy dataset of 50 observations with a cosine relationship.

\begin{lstlisting}
import numpy as np

x_train = np.linspace(-3, 3, num=50)
y_train = np.cos(x_train) + np.random.normal(0, 0.1, size=50)
x_train = x_train.astype(np.float32).reshape((50, 1))
y_train = y_train.astype(np.float32).reshape((50, 1))
\end{lstlisting}

\begin{figure}[h]
\centering
\includegraphics[width=3.5in]{images/getting-started-fig0.pdf}
\caption{Simulated data with a cosine relationship between $x$ and $y$.}
\end{figure}

Next, define a two-layer Bayesian neural network. Here, we
define the neural network manually with \texttt{tanh} nonlinearities.

\begin{lstlisting}[language=Python]
import tensorflow as tf
from edward.models import Normal

W_0 = Normal(mu=tf.zeros([1, 2]), sigma=tf.ones([1, 2]))
W_1 = Normal(mu=tf.zeros([2, 1]), sigma=tf.ones([2, 1]))
b_0 = Normal(mu=tf.zeros(2), sigma=tf.ones(2))
b_1 = Normal(mu=tf.zeros(1), sigma=tf.ones(1))

x = x_train
y = Normal(mu=tf.matmul(tf.tanh(tf.matmul(x, W_0) + b_0), W_1) + b_1,
           sigma=0.1)
\end{lstlisting}

Next, make inferences about the model from data. We will use variational
inference. Specify a normal approximation over the weights and biases.

\begin{lstlisting}[language=Python]
qW_0 = Normal(mu=tf.Variable(tf.zeros([1, 2])),
              sigma=tf.nn.softplus(tf.Variable(tf.zeros([1, 2]))))
qW_1 = Normal(mu=tf.Variable(tf.zeros([2, 1])),
              sigma=tf.nn.softplus(tf.Variable(tf.zeros([2, 1]))))
qb_0 = Normal(mu=tf.Variable(tf.zeros(2)),
              sigma=tf.nn.softplus(tf.Variable(tf.zeros(2))))
qb_1 = Normal(mu=tf.Variable(tf.zeros(1)),
              sigma=tf.nn.softplus(tf.Variable(tf.zeros(1))))
\end{lstlisting}

Defining \texttt{tf.Variable} allows the variational factors'
parameters to vary. They are all initialized at 0. The standard
deviation parameters are constrained to be greater than zero according
to a
softplus transformation\footnote{
The softplus function is defined as $\textrm{softplus}(x) =
\log(1+\exp(x))$.}.

Now, run variational inference with the
Kullback-Leibler divergence
in order to infer the model's latent variables given data.
We specify \texttt{1000} iterations.

\begin{lstlisting}[language=Python]
import edward as ed

inference = ed.KLqp({W_0: qW_0, b_0: qb_0,
                     W_1: qW_1, b_1: qb_1}, data={y: y_train})
inference.run(n_iter=1000)
\end{lstlisting}

Finally, criticize the model fit. Bayesian neural networks define a distribution
over neural networks, so we can perform a graphical check. Draw neural networks
from the inferred model and visualize how well it fits the data.

\begin{figure}[h]
\centering
\includegraphics[width=3.5in]{images/getting-started-fig1.pdf}
\caption{Posterior draws from the inferred Bayesian neural network.}
\end{figure}

The model has captured the cosine relationship between $x$ and $y$
in the observed domain.
