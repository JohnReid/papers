\section{Model Examples}
\label{appendix:model}

There are many examples available at \url{http://edwardlib.org},
including models, inference methods, and complete scripts.
Below we describe several model examples; \Cref{appendix:svi}
describes an inference example (stochastic variational inference);
\Cref{appendix:complete} describes complete scripts.

\subsection{Bayesian Neural Network for Classification}
\label{appendix:bnn}

A Bayesian neural network is a neural network with a prior
distribution on its weights.

Define the likelihood of an observation $(\mathbf{x}_n, y_n)$ with
binary label $y_n\in\{0,1\}$ as
\begin{align*}
  p(y_n \mid \mbW_0, \mbb_0, \mbW_1, \mbb_1 \;;\; \mathbf{x}_n)
  &=
  \operatorname{Bernoulli}(y_n \g \mathrm{NN}(\mathbf{x}_n\;;\;
  \mbW_0, \mbb_0, \mbW_1, \mbb_1)),
\end{align*}
where $\mathrm{NN}$ is a 2-layer neural network whose weights and biases form
the latent variables $\mbW_0, \mbb_0, \mbW_1, \mbb_1$.
%
Define the prior on the weights and biases to be the standard normal.
See \Cref{fig:bnn}. There are $N$ data points, $D$ features, and $H$
hidden units.

\begin{figure}[tb]
\begin{subfigure}{\columnwidth}
  \centering
  \input{img/bayesian_neural_network}
\end{subfigure}%
\\
\begin{subfigure}{\columnwidth}
\begin{lstlisting}[language=python]
W_0 = Normal(mu=tf.zeros([D, H]), sigma=tf.ones([D, H]))
W_1 = Normal(mu=tf.zeros([H, 1]), sigma=tf.ones([H, 1]))
b_0 = Normal(mu=tf.zeros(H), sigma=tf.ones(L))
b_1 = Normal(mu=tf.zeros(1), sigma=tf.ones(1))

x = tf.placeholder(tf.float32, [N, D])
y = Bernoulli(logits=tf.matmul(tf.nn.tanh(tf.matmul(x, W_0) + b_0), W_1) + b_1)
\end{lstlisting}
\end{subfigure}
\caption{Bayesian neural network for classification.}
\label{fig:bnn}
\end{figure}

\subsection{Dirichlet Process Mixture Model}
\label{appendix:dirichlet_process}

See \Cref{fig:dp}.

\begin{figure}[!h]
\begin{lstlisting}[language=python]
H = Normal(mu=tf.zeros(D), sigma=tf.ones(D))
mu = tf.pack([DirichletProcess(alpha=1.0, base=H) for _ in range(N)])
x = Normal(mu=mu, sigma=tf.ones(N))
\end{lstlisting}
The essential component defining the \texttt{DirichletProcess} random
variable is a stochastic while loop. We define it below.
\begin{lstlisting}[language=python]
def dirichlet_process(alpha):
  def cond(k, beta_k):
    flip = Bernoulli(p=beta_k)
    return tf.equal(flip, tf.constant(1))

  def body(k, beta_k):
    beta_k = beta_k * Beta(a=1.0, b=alpha)
    return k + 1, beta_k

  k = tf.constant(0)
  beta_k = Beta(a=1.0, b=alpha)
  stick_num, stick_beta = tf.while_loop(cond, body, loop_vars=[k, beta_k])
  return stick_num
\end{lstlisting}
\caption{Dirichlet process mixture model .}
\label{fig:dp}
\end{figure}

\subsection{Latent Dirichlet Allocation}
\label{appendix:lda}

See \Cref{fig:lda}.

\begin{figure}[!h]
\begin{subfigure}{0.45\columnwidth}
  \centering
  \input{img/lda}
\end{subfigure}%
\begin{subfigure}{0.55\columnwidth}
\begin{lstlisting}[language=python]
D = 50  # number of documents
N = [11502, 213, 1523, 1351, ...]  # words per doc
K = 10  # number of topics
V = 100000  # vocabulary size

theta = Dirichlet(alpha=tf.zeros([D, K]) + 0.1)
phi = Dirichlet(alpha=tf.zeros([K, V]) + 0.05)
z = [[0] * N] * D
w = [[0] * N] * D
for d in range(D):
  for n in range(N[d]):
    z[d][n] = Categorical(pi=theta[d, :])
    w[d][n] = Categorical(pi=phi[z[d][n], :])
\end{lstlisting}
\end{subfigure}
\caption{Latent Dirichlet allocation \citep{blei2003latent}.}
\label{fig:lda}
\end{figure}

\subsection{Gaussian Matrix Factorizationn}
\label{appendix:gaussian_mf}

See \Cref{fig:gaussian_mf}.

\begin{figure}[!h]
\begin{subfigure}{0.3\columnwidth}
  \centering
  \input{img/gaussian_mf}
\end{subfigure}%
\begin{subfigure}{0.7\columnwidth}
\begin{lstlisting}[language=python]
N = 10
M = 10
K = 5  # latent dimension

U = Normal(mu=tf.zeros([M, K]), sigma=tf.ones([M, K]))
V = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))
Y = Normal(mu=tf.matmul(U, V, transpose_b=True), sigma=tf.ones([N, M]))
\end{lstlisting}
\end{subfigure}
\caption{Gaussian matrix factorization.}
\label{fig:gaussian_mf}
\end{figure}

\section{Inference Example: Stochastic Variational Inference}
\label{appendix:svi}

In the subgraph setting, we do data subsampling while working with a
subgraph of the full model. This setting is necessary when the data
and model do not fit in memory.
It is scalable in that both the
algorithm's computational complexity (per iteration) and memory
complexity are independent of the data set size.

For the code, we use the running example, a mixture model
described in \Cref{fig:hierarchical_model_example}.
\begin{lstlisting}[language=Python]
N = 10000000  # data set size
D = 2  # data dimension
K = 5  # number of clusters
\end{lstlisting}
The model is
\begin{equation*}
p(\mbx, \mathbf{z}, \beta)
= p(\beta) \prod_{n=1}^N p(z_n \mid \beta) p(x_n \mid z_n, \beta).
\end{equation*}
To avoid memory issues, we work on only a subgraph of the model,
\begin{equation*}
p(\mbx, \mathbf{z}, \beta)
= p(\beta) \prod_{m=1}^M p(z_m \mid \beta) p(x_m \mid z_m, \beta)
\end{equation*}
\begin{lstlisting}[language=Python]
M = 128  # mini-batch size

beta = Normal(mu=tf.zeros([K, D]), sigma=tf.ones([K, D]))
z = Categorical(logits=tf.zeros([M, K]))
x = Normal(mu=tf.gather(beta, z), sigma=tf.ones([M, D]))
\end{lstlisting}
Assume the variational model is
\begin{equation*}
q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{n=1}^N q(z_n \mid \beta; \gamma_n),
\end{equation*}
parameterized by $\{\lambda, \{\gamma_n\}\}$.
Again, we work on only a subgraph of the model,
\begin{equation*}
q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{m=1}^M q(z_m \mid \beta; \gamma_m).
\end{equation*}
parameterized by $\{\lambda, \{\gamma_m\}\}$. Importantly, only $M$
parameters are stored in memory for $\{\gamma_m\}$ rather than $N$.
\begin{lstlisting}[language=Python]
qbeta = Normal(mu=tf.Variable(tf.zeros([K, D])),
               sigma=tf.nn.softplus(tf.Variable(tf.zeros[K, D])))
qz_variables = tf.Variable(tf.zeros([M, K]))
qz = Categorical(logits=qz_variables)
\end{lstlisting}
We instantiate the inference algorithm to perform inference over
$\beta$ and the subset of $\mathbf{z}$.
We use \texttt{KLqp}, a variational method that minimizes
the divergence measure $\operatorname{KL}(q\gg p)$
\citep{jordan1999introduction}.
We also pass in a TensorFlow placeholder \texttt{x_ph} for the data,
so we can change the data at each step. (Alternatively,
batch tensors in TensorFlow can be used.)
\begin{lstlisting}[language=Python]
x_ph = tf.placeholder(tf.float32, [M])
inference = ed.KLqp({beta: qbeta, z: qz}, data={x: x_ph})
\end{lstlisting}
We initialize the algorithm with the \texttt{scale} argument, so that
computation on \texttt{z} and \texttt{x} will be scaled appropriately.
This enables unbiased estimates for stochastic gradients.
\begin{lstlisting}[language=Python]
inference.initialize(scale={x: float(N) / M, z: float(N) / M})
\end{lstlisting}
We now run the algorithm, assuming there is a \texttt{next_batch}
function which provides the next batch of data.
\begin{lstlisting}[language=Python]
qz_init = tf.initialize_variables([qz_variables])
for _ in range(10000):
  x_batch = next_batch(size=M)
  for _ in range(10): # run multiple updates for each batch
    inference.update(feed_dict={x_ph: x_batch})
  # reinitialize the local factors
  qz_init.run()
\end{lstlisting}
After each iteration, we reinitialize the parameters for
$q(\mathbf{z}\mid\beta; \mbgamma)$; this is because we do inference on a new
set of local variational factors for each batch.
This demo readily applies to other stochastic inference
algorithms such as stochastic gradient Langevin dynamics: simply
replace \texttt{qbeta} and \texttt{qz} with \texttt{Empirical} random
variables; then call \texttt{ed.SGLD} instead of \texttt{ed.KLqp}.

Note that if the data and model fit in memory but you'd still like to
perform data subsampling for fast inference, we recommend not defining
subgraphs. You can reify the full model, and simply index the local
variables with a placeholder. The placeholder is fed at runtime to
determine which of the local variables to update at a time. (For more
details, see the website's API.)

\section{Complete Examples}
\label{appendix:complete}

\subsection{Variational Auto-encoder}
\label{appendix:vae}

See \Cref{fig:appendix_vae}.

\begin{figure}[!h]
\begin{lstlisting}[language=python]
import edward as ed
import tensorflow as tf

from edward.models import Bernoulli, Normal
from scipy.misc import imsave
from tensorflow.contrib import slim
from tensorflow.examples.tutorials.mnist import input_data

M = 100  # batch size during training
d = 2  # latent variable dimension

# Probability model (subgraph)
z = Normal(mu=tf.zeros([M, d]), sigma=tf.ones([M, d]))
h = slim.fully_connected(z, 256)
x = Bernoulli(logits=slim.fully_connected(h, 28 * 28, activation_fn=None))

# Variational model (subgraph)
x_ph = tf.placeholder(tf.float32, [M, 28 * 28])
qh = slim.fully_connected(x_ph, 256)
qz = Normal(mu=slim.fully_connected(qh, d, activation_fn=None),
            sigma=slim.fully_connected(qh, d, activation_fn=tf.nn.softplus))

# Bind p(x, z) and q(z | x) to the same TensorFlow placeholder for x.
mnist = input_data.read_data_sets("data/mnist", one_hot=True)
data = {x: x_ph}

inference = ed.KLqp({z: qz}, data)
optimizer = tf.train.RMSPropOptimizer(0.01, epsilon=1.0)
inference.initialize(optimizer=optimizer)

tf.initialize_all_variables().run()

n_epoch = 100
n_iter_per_epoch = 1000
for _ in range(n_epoch):
  for _ in range(n_iter_per_epoch):
    x_train, _ = mnist.train.next_batch(M)
    info_dict = inference.update(feed_dict={x_ph: x_train})

  # Generate images.
  imgs = x.value().eval()
  for m in range(M):
    imsave("img/%d.png" % m, imgs[m].reshape(28, 28))
\end{lstlisting}
\caption{Complete script for a \gls{VAE}
\citep{kingma2014autoencoding} with batch training.
It generates MNIST digits after every 1000 updates.}
\label{fig:appendix_vae}
\end{figure}

\subsection{Generative Model for Word Embeddings}
\label{appendix:ef_emb}

\begin{figure}[!h]
\begin{lstlisting}[language=python]
import edward as ed
import tensorflow as tf

from edward.models import Bernoulli, Normal, PointMass

N = 581238  # number of total words
M = 128  # batch size during training
K = 100  # number of factors
ns = 3  # number of negative samples
cs = 4  # context size
L = 50000  # vocabulary size

# Prior over embedding vectors
p_rho = Normal(mu=tf.zeros([M, K]),
               sigma=tf.sqrt(N) * tf.ones([M, K]))
n_rho = Normal(mu=tf.zeros([M, ns, K]),
               sigma=tf.sqrt(N) * tf.ones([M, ns, K]))

# Prior over context vectors
ctx_alphas = Normal(mu=tf.zeros([M, cs, K]),
                    sigma=tf.sqrt(N)*tf.ones([M, cs, K]))

# Conditional likelihoods
ctx_sum = tf.reduce_sum(ctx_alphas, [1])
p_eta = tf.expand_dims(tf.reduce_sum(tf.mul(p_rho, ctx_sum), -1),1)
n_eta = tf.reduce_sum(n_rho * tf.tile(tf.expand_dims(ctx_sum, 1), [1, ns, 1]), -1)
y_pos = Bernoulli(logits=p_eta)
y_neg = Bernoulli(logits=n_eta)

# placeholders for batch training
p_idx = tf.placeholder(tf.int32, [M, 1])
n_idx = tf.placeholder(tf.int32, [M, ns])
ctx_idx = tf.placeholder(tf.int32, [M, cs])

# Variational parameters (embedding vectors)
rho_params = tf.Variable(tf.random_normal([L, K]))
alpha_params = tf.Variable(tf.random_normal([L, K]))

# Variational distribution on embedding vectors
q_p_rho = PointMass(params=tf.squeeze(tf.gather(rho_params, p_idx)))
q_n_rho = PointMass(params=tf.gather(rho_params, n_idx))
q_alpha = PointMass(params=tf.gather(alpha_params, ctx_idx))

inference = ed.MAP(
  {p_rho: q_p_rho, n_rho: q_n_rho, ctx_alphas: q_alpha},
  data={y_pos: tf.ones((M, 1)), y_neg: tf.zeros((M, ns))})

inference.initialize()
tf.initialize_all_variables().run()

for _ in range(inference.n_iter):
  targets, windows, negatives = next_batch(M)  # a function to generate data
  info_dict = inference.update(feed_dict={p_idx: targets, ctx_idx: windows, n_idx: negatives})
  inference.print_progress(info_dict)
\end{lstlisting}
\caption{Exponential family embedding for binary data \citep{rudolph2016exponential}. Here, \gls{MAP}
is used to maximize the total sum of conditional log-likelihoods and log-priors.}
\label{fig:ef_emb}
\end{figure}

See \Cref{fig:ef_emb}.
%
This example uses data subsampling (\Cref{appendix:svi}). The
priors and conditional likelihoods are defined only for a minibatch of
data. Similarly the variational model only models the embeddings used
in a given minibatch. TensorFlow variables contain the embedding
vectors for the entire vocabulary. TensorFlow placeholders ensure that
the correct embedding vectors are used as variational parameters for a
given minibatch.

The Bernoulli variables \texttt{y_pos} and \texttt{y_neg} are fixed to
be $1$'s and $0$'s respectively. They model whether a word is indeed the target word
for a given context window or has been drawn as a negative sample.
Without regularization (via priors), the objective we optimize is
identical to negative sampling.
