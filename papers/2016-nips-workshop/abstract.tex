\begin{abstract}
We propose Edward, a new Turing-complete \acrlong{PPL} which builds
on two compositional representations---random variables and
inference. We show how to integrate our language into existing
computational graph frameworks such as TensorFlow; this provides
significant speedups over existing probabilistic systems.
We also show how Edward makes it easy to fit the same model using a
variety of composable inference methods, ranging from point estimation, to
variational inference, to \acrshort{MCMC}. By treating inference as a
first class citizen, on a par with modeling, we show that
probabilistic programming
can be as computationally efficient and flexible as traditional deep
learning.
For example, we show how to reuse the modeling representation
within inference to design rich variational models and \acrlongpl{GAN}.
\end{abstract}
