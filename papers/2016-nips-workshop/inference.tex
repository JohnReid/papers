\section{Compositional Representations for Inference}
\label{sec:inference}

We have described random variables, a representation for building rich
probabilistic programs over computational graphs.  We now describe a
compositional representation for inference.

For inference,
we desire two criteria: (a) support for many classes of inference,
where the form of the inferred posterior depends on the
algorithm; and (b) invariance of inference under the computational
graph, that is, the posterior can be further composed as part of
another model.

To explain our approach to this problem,
we will use a simple  hierarchical model
$p(\mbx, \mbz, \beta)$,
represented in Figure~\ref{fig:hierarchical_model_example},
as a running example. The ideas extend to more expressive
programs.

\begin{figure}[!h]
\begin{subfigure}{0.35\columnwidth}
  \centering
  \input{img/hierarchical_model_full}
\end{subfigure}%
\begin{subfigure}{0.6\columnwidth}
  \centering
\begin{lstlisting}[language=python]
N = 10000  # number of data points
D = 2  # data dimension
K = 5  # number of clusters

beta = Normal(mu=tf.zeros([K, D]), sigma=tf.ones([K, D]))
z = Categorical(logits=tf.zeros([N, K]))
x = Normal(mu=tf.gather(beta, z), sigma=tf.ones([N, D]))
\end{lstlisting}
\end{subfigure}
\caption{Hierarchical model: (left) graphical model; (right)
probabilistic program. It is a mixture of Gaussians over
$D$-dimensional data $\{x_n\}\in\mathbb{R}^{N\times D}$. There are $K$
latent cluster means
$\beta\in\mathbb{R}^{K\times D}$.}
\label{fig:hierarchical_model_example}
\end{figure}

\subsection{Inference as Stochastic Graph Optimization}

Given data $\mbx_{\text{train}}$, inference aims to calculate the
posterior
$p(\mathbf{z}, \beta\mid \mathbf{x}_{\text{train}}; \mbtheta)$, where
$\mbtheta$ are any model parameters that we will compute point estimates
for.\footnote{%
For example, we could replace \texttt{x}'s \texttt{sigma}
argument with \texttt{tf.exp(tf.Variable(0.0))*tf.ones([N, D])}. This
defines a model parameter initialized at 0 and positive-constrained.}
We formalize this as the problem,
\begin{equation*}
\min_{\mblambda,\mbtheta}
\mathcal{L}(
p(\mathbf{z}, \beta\mid \mathbf{x}_{\text{train}}; \mbtheta),~
q(\mathbf{z}, \beta; \mblambda)
),
\end{equation*}
where $q(\mathbf{z}, \beta; \mblambda)$ is an approximation
to the posterior $p(\mathbf{z}, \beta\g \mbx_{\text{train}})$,
and $\mathcal{L}(\cdot)$ is some loss function with respect to $p$ and
$q$.

The choice of approximation $q$, loss $\mathcal{L}$, and rules to update
parameters $\{\mbtheta,\mblambda\}$ are specified by an inference algorithm.
(Note $q$ can be nonparametric, such as a point or a collection of
samples.)

In our language, we write this problem as follows:
\begin{lstlisting}[language=python]
inference = ed.Inference({beta: qbeta, z: qz}, data={x: x_train})
\end{lstlisting}
where \texttt{qbeta} and \texttt{qz} are random variables defined to
approximate the posterior. \texttt{Inference} is an abstract class which
takes two inputs: a collection of latent variables, with model
variables bound to posterior variables; and a collection of observed
variables, with model variables bound to data.  Calling
\texttt{inference.initialize()} builds a computational graph to update
$\{\mbtheta,\mblambda\}$. Calling \texttt{inference.update()} runs
this computation once to update
$\{\mbtheta,\mblambda\}$; we call the method in a loop until
convergence. Below we will derive subclasses of \texttt{Inference}
to represent many methods.

\subsection{Representing Classes of Inference}

We show how to leverage the above to represent a broad class of
inference methods.

In variational inference, the idea is to posit a family of approximating
distributions and to find the closest member in the family to the
posterior \citep{jordan1999introduction}.
We build the variational family in the graph;
see \Cref{fig:inference} (left).
The variational family has mutable
variables representing its parameters $\mblambda=\{\pi,\mu,\sigma\}$,
where
$q(\beta;\mu,\sigma) = \operatorname{Normal}(\beta; \mu,\sigma)$
and
$q(\mbz;\pi) = \operatorname{Categorical}(\mbz;\pi)$.

\begin{figure}[!h]
\begin{subfigure}{0.5\columnwidth}
  \centering
\begin{lstlisting}[language=Python]
qbeta = Normal(
  mu=tf.Variable(tf.zeros([K, D])),
  sigma=tf.exp(tf.Variable(tf.zeros[K, D])))
qz = Categorical(
  logits=tf.Variable(tf.zeros[N, K]))

inference = ed.VariationalInference(
  {beta: qbeta, z: qz}, data={x: x_train})
\end{lstlisting}
\end{subfigure}%
\begin{subfigure}{0.5\columnwidth}
  \centering
\begin{lstlisting}[language=Python]
T = 10000  # number of samples
qbeta = Empirical(
  params=tf.Variable(tf.zeros([T, K, D]))
qz = Empirical(
  params=tf.Variable(tf.zeros([T, N]))

inference = ed.MonteCarlo(
  {beta: qbeta, z: qz}, data={x: x_train})
\end{lstlisting}
\end{subfigure}
\caption{(left) Variational inference. (right) Monte Carlo.}
\label{fig:inference}
\end{figure}

Specific variational inference algorithms inherit from
the \texttt{VariationalInference} class to define their own methods, such as a
loss function and gradient. For example, we represent \gls{MAP} estimation with
an approximating family (\texttt{qbeta} and \texttt{qz}) of
\texttt{PointMass} random variables,
i.e., with all probability mass concentrated at a point. \texttt{MAP}
inherits from \texttt{VariationalInference} and defines a loss
function and update rules; it leverages existing optimizers inside TensorFlow.

Monte Carlo approximates the posterior using samples
\citep{robert1999monte}. We represent Monte Carlo as inference where
the approximating family is an empirical distribution,
$q(\beta; \{\beta^{(t)}\}) = \frac{1}{T}\sum_{t=1}^T \delta(\beta,
\beta^{(t)})$
and
$q(\mbz; \{\mbz^{(t)}\}) = \frac{1}{T}\sum_{t=1}^T \delta(\mbz,
\mbz^{(t)})$. The parameters are $\mblambda=\{\beta^{(t)},\mbz^{(t)}\}$.
See \Cref{fig:inference} (right).
Monte Carlo
algorithms proceed by updating one sample $\beta^{(t)},\mbz^{(t)}$ at a time in the empirical
approximation.
Specific \glsunset{MC}\gls{MC} samplers determine the update rules;
they can leverage gradients and graph structure, where applicable.

The approach also extends to exact inference. We are developing a
subpackage that does symbolic algebra on the deterministic and
stochastic nodes in the computational graph; this uncovers conjugacy
relationships between exponential-family random variables. This will
allow users to integrate out variables and automatically derive
classical Gibbs and mean-field updates \citep{bishop2006pattern} without
tedious algebraic manipulation.

\subsection{Composing Inferences}

Core to Edward's design is that inference can be written as a collection
of separate inference programs. Below we demonstrate variational EM,
with an (approximate) E-step over local variables and an M-step over
global variables. We alternate with one update of each \citep{neal1993new}.
\begin{lstlisting}[language=Python]
qbeta = PointMass(params=tf.Variable(tf.zeros([K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference_e = ed.VariationalInference({z: qz}, data={x: x_data, beta: qbeta})
inference_m = ed.MAP({beta: qbeta}, data={x: x_data, z: qz})

for _ in range(10000):
  inference_e.update()
  inference_m.update()
\end{lstlisting}
This extends to many other cases, such as
exact EM for exponential families,
contrastive divergence \citep{hinton2002training},
pseudo-marginal methods \citep{andrieu2009pseudo},
and
Gibbs sampling within variational inference \citep{wang2012truncation}.
We can also write message passing algorithms, which work over a
collection of local inference problems
\citep{koller2009probabilistic}.
%
For example, the local inference can be exact for classical message
passing or minimize $\text{KL}(p\gg q)$ for expectation
propagation \citep{minka2001expectation}.
In \Cref{appendix:svi}, we demonstrate this approach for
stochastic variational inference.
