\section{End-to-end Examples}
\label{sec:examples}

\subsection{Bayesian Linear Regression}

In supervised learning, the task is to infer hidden structure from
labeled data, comprised of training examples $\{(x_n, y_n)\}$.
Regression (typically) means the output $y$ takes continuous values.

\subsubsection{Data}

Simulate training and test sets of $500$ data points. They comprise
pairs of inputs $\mathbf{x}_n\in\mathbb{R}^{5}$ and outputs
$y_n\in\mathbb{R}$. They have a linear dependence of
\begin{align*}
  \mathbf{w}_{\text{true}}
  &=
  (-1.25, 4.51, 2.32, 0.99, -3.44).
\end{align*}
with normally distributed noise.

\begin{lstlisting}
def build_toy_dataset(N, w, noise_std=0.1):
  D = len(w)
  x = np.random.randn(N, D).astype(np.float32)
  y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)
  return x, y

N = 500  # number of  data points
D = 5  # number of  features

w_true = 10 * (np.random.rand(D) - 0.5)
X_train, y_train = build_toy_dataset(N, w_true)
X_test, y_test = build_toy_dataset(N, w_true)
\end{lstlisting}

\subsubsection{Model}

Posit the model as Bayesian linear regression. It relates
outputs $y\in\mathbb{R}$, also known as the response, given
a vector of inputs
$\mathbf{x}\in\mathbb{R}^D$, also known as the features or covariates.
The model assumes a
linear relationship between these two random variables
\citep{murphy2012machine}.

For a set of $N$ data points $(\mathbf{X},\mathbf{y})=\{(\mathbf{x}_n, y_n)\}$,
the model posits the following conditional relationships:
\begin{align*}
  p(\mathbf{w})
  &=
  \text{Normal}(\mathbf{w} \mid \mathbf{0}, \sigma_w^2\mathbf{I}),
  \\[1.5ex]
  p(b)
  &=
  \text{Normal}(b \mid 0, \sigma_b^2),
  \\
  p(\mathbf{y} \mid \mathbf{w}, b, \mathbf{X})
  &=
  \prod_{n=1}^N
  \text{Normal}(y_n \mid \mathbf{x}_n^\top\mathbf{w} + b, \sigma_y^2).
\end{align*}
The latent variables are the linear model's weights $\mathbf{w}$ and
intercept $b$, also known as the bias.
Assume $\sigma_w^2,\sigma_b^2$ are known prior variances and $\sigma_y^2$ is a
known likelihood variance. The mean of the likelihood is given by a
linear transformation of the inputs $\mathbf{x}_n$.

\begin{lstlisting}
X = tf.placeholder(tf.float32, [N, D])
w = Normal(mu=tf.zeros(D), sigma=tf.ones(D))
b = Normal(mu=tf.zeros(1), sigma=tf.ones(1))
y = Normal(mu=ed.dot(X, w) + b, sigma=tf.ones(N))
\end{lstlisting}

\subsubsection{Inference}

We now turn to inferring the posterior using variational inference.
Define the variational model to be a fully factorized normal across
the weights.
\begin{lstlisting}
qw = Normal(mu=tf.Variable(tf.random_normal([D])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))
qb = Normal(mu=tf.Variable(tf.random_normal([1])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))
\end{lstlisting}

Run variational inference with the Kullback-Leibler divergence, using a
default of $1000$ iterations.
\begin{lstlisting}
inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})
inference.run()
\end{lstlisting}
In this case \texttt{KLqp} defaults to minimizing the
$\text{KL}(q\|p)$ divergence measure using the reparameterization
gradient.
Minimizing this divergence metric is equivalent to maximizing the
evidence lower bound (\textsc{elbo}). Figure\nobreakspace \ref{fig:supervised-elbo} shows the
progression of the \textsc{elbo} across iterations; variational inference
appears to converge in approximately 200 iterations.

\begin{figure}[htb]
\centering
\input{tikz/elbo}
\caption{The evidence lower bound (\textsc{elbo}) as a function of iterations.
Variational inference maximizes this quantity iteratively; in this case, the
algorithm appears to have converged in approximately 200 iterations.}
\label{fig:supervised-elbo}
\end{figure}

Figure\nobreakspace \ref{fig:supervised-betas} shows the resulting posteriors from  variational
inference. We plot the marginal posteriors for each component of the vector of
coefficients $\beta$. The vertical lines indicate the ``true'' values of the
coefficients that we simulated above.

\begin{figure}[htb]
\centering
\input{tikz/beta0}
\input{tikz/beta1}

\medskip

\input{tikz/beta2}
\input{tikz/beta3}

\medskip

\input{tikz/beta4}
\caption{Visualization of the inferred marginal posteriors for Bayesian linear
regression. The gray bars indicate the simulated ``true'' value for each
component of the coefficient vector.}
\label{fig:supervised-betas}
\end{figure}

\subsubsection{Criticism}

A standard evaluation in regression is to calculate point-based evaluations on
held-out ``testing'' data. We do this first by forming the posterior predictive
distribution.
\begin{lstlisting}
y_post = Normal(mu=ed.dot(X, qw.mean()) + qb.mean(), sigma=tf.ones(N))
\end{lstlisting}

With this we can evaluate various point-based quantities using the posterior
predictive.
\begin{lstlisting}
print(ed.evaluate('mean_squared_error', data={X: X_test, y_post: y_test}))
> 0.012107

print(ed.evaluate('mean_absolute_error', data={X: X_test, y_post: y_test}))
> 0.0867875
\end{lstlisting}

The trained model makes predictions with low mean squared error
(relative to the magnitude of the output).

Edward supports another class of criticism techniques called
\glspl{PPC}.
The simplest \gls{PPC} works by applying a test statistic on new data
generated from the posterior predictive, such as
$T(\mathbf{x}_\text{new}) = \max(\mathbf{x}_\text{new})$.
Applying $T(\mathbf{x}_\text{new})$ to
new data over many data replications induces a distribution of the test
statistic, $\textsc{ppd}(T)$. We compare
this distribution to the test statistic applied to the original dataset
$T(\mathbf{x})$.

Calculating \glspl{PPC} in Edward is straightforward.
\begin{lstlisting}
def T(xs, zs):
  return tf.reduce_max(xs[y_post])

ppc_max = ed.ppc(T, data={X: X_train, y_post: y_train})
\end{lstlisting}
This calculates the test statistic on both the original dataset as well as on
data replications generated from teh posterior predictive distribution.
Figure \ref{fig:supervised-ppc} shows three visualizations of different \glspl
{PPC}; the
plotted posterior predictive distributions are kernel density estimates from
$N=500$ data replications.


\begin{figure}[htb]
\centering
\input{tikz/T_min}
\input{tikz/T_max}

\medskip

\input{tikz/T_mean}
\glsreset{PPC}
\caption{Examples of \glspl{PPC} for Bayesian linear regression. }
\label{fig:supervised-ppc}
\end{figure}





\subsection{Logistic and Neural Network Classification}

In supervised learning, the task is to infer hidden structure from
labeled data, comprised of training examples $\{(x_n, y_n)\}$.
Classification means the output $y$ takes discrete values.


\subsubsection{Data}

We study a two-dimensional simulated dataset with a nonlinear decision
boundary. We simulate $100$ datapoints using the following snippet.
\begin{lstlisting}
from scipy.stats import logistic

N = 100  # number of data points
D = 2  # number of features

px1 = np.linspace(-3, 3, 50)
px2 = np.linspace(-3, 3, 50)
px1_m, px2_m = np.mgrid[-3:3:50j, -3:3:50j]

xeval = np.vstack((px1_m.flatten(), px2_m.flatten())).T
x_viz = tf.constant(np.array(xeval, dtype='float32'))

def build_toy_dataset(N):
  x = xeval[np.random.randint(xeval.shape[0],size=N), :]
  y = bernoulli.rvs(p=logistic.cdf( 5 * x[:, 0]**2 + 5 * x[:, 1]**3 ))
  return x, y

x_train, y_train = build_toy_dataset(N)
\end{lstlisting}

Figure\nobreakspace \ref{fig:lr_data} shows the data, colored by label. The red point near the
origin makes this a challenging dataset for classification models that assume a
linear decision boundary.
\begin{figure}[!htbp]
\centering
\includegraphics[width=2.5in]{images/lr_data.pdf}
\caption{Simulated data for classification. Positive and negative measurements
colored by label.}
\label{fig:lr_data}
\end{figure}


\subsubsection{Model: Bayesian Logistic Regression}

We begin with a popular classification model: logistic regression. This model
relates outputs $y\in\{0,1\}$, also known as the response, given
a vector of inputs $\mathbf{x}\in\mathbb{R}^D$, also known as the features or
covariates. The model assumes a latent linear relationship between these two random
variables \citep{gelman2013bayesian}.

The likelihood of each datapoint is a Bernoulli with probability
\begin{align*}
\Pr(y_n=1)
  &=
  \text{logistic}
  \left(
  \mathbf{x}^\top \mathbf{w} + b
  \right).
\end{align*}
We posit priors on the latent variables $\mathbf{w}$ and $b$ as
\begin{align*}
  p(\mathbf{w})
  &=
  \text{Normal}(\mathbf{w} \mid \mathbf{0}, \sigma_w^2\mathbf{I}),
  \\
  p(b)
  &=
  \text{Normal}(b \mid 0, \sigma_b^2).
\end{align*}

This model is
easy to specify in Edward's native language.

\begin{lstlisting}
W = Normal(mu=tf.zeros(D), sigma=tf.ones(D))
b = Normal(mu=tf.zeros(1), sigma=tf.ones(1))

x = tf.cast(x_train, dtype=tf.float32)
y = Bernoulli(logits=(ed.dot(x, W) + b))
\end{lstlisting}


\subsubsection{Inference}

Here, we perform variational inference. Define the variational model to be a
fully factorized normal
\begin{lstlisting}
qW = Normal(mu=tf.Variable(tf.random_normal([D])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))
qb = Normal(mu=tf.Variable(tf.random_normal([1])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))
\end{lstlisting}

Run variational inference with the Kullback-Leibler divergence for $1000$ iterations.
\begin{lstlisting}
inference = ed.KLqp({W: qW, b: qb}, data={y: y_train})
inference.run(n_iter=1000, n_print=100, n_samples=5)
\end{lstlisting}
In this case
\texttt{KLqp} defaults to minimizing the
$\text{KL}(q\|p)$ divergence measure using the reparameterization
gradient.


\subsubsection{Criticism}

The first thing to look at are point-wise evaluations on the training dataset.

First form a plug-in estimate of the posterior predictive distribution.
\begin{lstlisting}
y_post = ed.copy(y, {W: qW.mean(), b: qb.mean()})
\end{lstlisting}

Then evaluate predictive accuracy
\begin{lstlisting}
print('Plugin estimate of posterior predictive log accuracy on training data:')
print(ed.evaluate('log_lik', data={x: x_train, y_post: y_train}))
> -3.12

print('Binary accuracy on training data:')
print(ed.evaluate('binary_accuracy', data={x: x_train, y_post: y_train}))
> 0.71
\end{lstlisting}

Figure\nobreakspace \ref {fig:lr_linear} shows the posterior label probability evaluated on a grid.
As expected, logistic regression attempts to fit a linear boundary between the
two label classes. Can a non-linear model do better?
\begin{figure}[!htb]
\centering
\includegraphics[width=2.5in]{images/lr_linear.pdf}
\caption{Logistic regression struggles to separate the measurements.}
\label{fig:lr_linear}
\end{figure}

\subsubsection{Model: Bayesian Neural Network Classification}

Consider parameterizing the label probability using a neural network; this
model is not limited to a linear relationship to the inputs $\mathbf{x}$, as in
logistic regression.

The model posits a likelihood for each observation $(\mathbf{x}_n,
y_n)$ as
\begin{align*}
\Pr(y_n=1)
  &=
  \text{logistic}
  \left(
  \text{NN}(\mathbf{x}_n \;;\; \mathbf{z})
  \right),
\end{align*}
where NN is a neural network and the latent random variable $\mathbf{z}$
contains its weights and biases.

We can specify a Bayesian neural network in Edward as follows. Here we specify a
fully connected two-layer network with two nodes in each layer; we posit
standard normal priors on all weights and biases.
\begin{lstlisting}
def neural_network(x, W_0, W_1, W_2, b_0, b_1, b_2):
    h = tf.nn.tanh(tf.matmul(x, W_0) + b_0)
    h = tf.nn.tanh(tf.matmul(h, W_1) + b_1)
    h = tf.matmul(h, W_2) + b_2
    return tf.reshape(h, [-1])

H = 2  # number of hidden units in each layer

W_0 = Normal(mu=tf.zeros([D, H]), sigma=tf.ones([D, H]))
W_1 = Normal(mu=tf.zeros([H, H]), sigma=tf.ones([H, H]))
W_2 = Normal(mu=tf.zeros([H, 1]), sigma=tf.ones([H, 1]))
b_0 = Normal(mu=tf.zeros(H), sigma=tf.ones(H))
b_1 = Normal(mu=tf.zeros(H), sigma=tf.ones(H))
b_2 = Normal(mu=tf.zeros(1), sigma=tf.ones(1))

x = tf.cast(x_train, dtype=tf.float32)
y = Bernoulli(logits=neural_network(x, W_0, W_1, W_2, b_0, b_1, b_2))
\end{lstlisting}

\subsubsection{Inference}

Similar to the above, we perform variational inference. Define the variational
model to be a fully factorized normal over all latent variables
\begin{lstlisting}
qW_0 = Normal(mu=tf.Variable(tf.random_normal([D, H])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D, H]))))
qW_1 = Normal(mu=tf.Variable(tf.random_normal([H, H])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([H, H]))))
qW_2 = Normal(mu=tf.Variable(tf.random_normal([H, 1])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([H, 1]))))
qb_0 = Normal(mu=tf.Variable(tf.random_normal([H])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([H]))))
qb_1 = Normal(mu=tf.Variable(tf.random_normal([H])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([H]))))
qb_2 = Normal(mu=tf.Variable(tf.random_normal([1])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))
\end{lstlisting}

Run variational inference for $1000$ iterations.
\begin{lstlisting}
inference = ed.KLqp({W_0: qW_0, b_0: qb_0,
                     W_1: qW_1, b_1: qb_1,
                     W_2: qW_2, b_2: qb_2}, data={y: y_train})
inference.run(n_iter=1000, n_print=100, n_samples=5)
\end{lstlisting}

\subsubsection{Criticism}

Again, we form a plug-in estimate of the posterior predictive distribution.
\begin{lstlisting}
y_post = ed.copy(y, {W_0: qW_0.mean(), b_0: qb_0.mean(),
                     W_1: qW_1.mean(), b_1: qb_1.mean(),
                     W_2: qW_2.mean(), b_1: qb_2.mean()})
\end{lstlisting}

Both predictive accuracy metrics look better.
\begin{lstlisting}
print('Plugin estimate of posterior predictive log accuracy on training data:')
print(ed.evaluate('log_lik', data={x: x_train, y_post: y_train}))
> -0.170941

print('Binary accuracy on training data:')
print(ed.evaluate('binary_accuracy', data={x: x_train, y_post: y_train}))
> 0.81
\end{lstlisting}

Figure\nobreakspace \ref{fig:lr_nn} shows the posterior label probability evaluated on a grid.
The neural network has captured the nonlinear decision boundary between the two
label classes.
\begin{figure}[!htb]
\centering
\includegraphics[width=2.5in]{images/lr_nn.pdf}
\caption{A Bayesian neural network does a better job of separating the two
label classes.}
\label{fig:lr_nn}
\end{figure}
